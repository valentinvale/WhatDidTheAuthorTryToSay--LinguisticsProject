{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seleniumbase\n",
      "  Downloading seleniumbase-4.33.12-py3-none-any.whl.metadata (86 kB)\n",
      "     ---------------------------------------- 0.0/86.0 kB ? eta -:--:--\n",
      "     --------- ---------------------------- 20.5/86.0 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 86.0/86.0 kB 806.4 kB/s eta 0:00:00\n",
      "Collecting pip>=24.3.1 (from seleniumbase)\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting packaging>=24.2 (from seleniumbase)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting setuptools>=75.6.0 (from seleniumbase)\n",
      "  Downloading setuptools-75.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting wheel>=0.45.1 (from seleniumbase)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: attrs>=24.3.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (24.3.0)\n",
      "Collecting certifi>=2024.12.14 (from seleniumbase)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting exceptiongroup>=1.2.2 (from seleniumbase)\n",
      "  Downloading exceptiongroup-1.2.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting websockets>=14.1 (from seleniumbase)\n",
      "  Downloading websockets-14.1-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting filelock>=3.16.1 (from seleniumbase)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fasteners>=0.19 (from seleniumbase)\n",
      "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting mycdp>=1.1.0 (from seleniumbase)\n",
      "  Downloading mycdp-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pynose>=1.5.3 (from seleniumbase)\n",
      "  Downloading pynose-1.5.3-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting platformdirs>=4.3.6 (from seleniumbase)\n",
      "  Downloading platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (4.12.2)\n",
      "Collecting sbvirtualdisplay>=1.4.0 (from seleniumbase)\n",
      "  Downloading sbvirtualdisplay-1.4.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting MarkupSafe>=3.0.2 (from seleniumbase)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting Jinja2>=3.1.5 (from seleniumbase)\n",
      "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting six>=1.17.0 (from seleniumbase)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting parse>=1.20.2 (from seleniumbase)\n",
      "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting parse-type>=0.6.4 (from seleniumbase)\n",
      "  Downloading parse_type-0.6.4-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (0.4.6)\n",
      "Collecting pyyaml>=6.0.2 (from seleniumbase)\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting pygments>=2.18.0 (from seleniumbase)\n",
      "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pyreadline3>=3.5.3 (from seleniumbase)\n",
      "  Downloading pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting tabcompleter>=1.4.0 (from seleniumbase)\n",
      "  Downloading tabcompleter-1.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting pdbp>=1.6.1 (from seleniumbase)\n",
      "  Downloading pdbp-1.6.1-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting idna==3.10 (from seleniumbase)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: chardet==5.2.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (5.2.0)\n",
      "Collecting charset-normalizer==3.4.1 (from seleniumbase)\n",
      "  Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl.metadata (36 kB)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.26.20 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (2.1.0)\n",
      "Collecting requests==2.32.3 (from seleniumbase)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: sniffio==1.3.1 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (1.3.1)\n",
      "Requirement already satisfied: h11==0.14.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (0.14.0)\n",
      "Requirement already satisfied: outcome==1.3.0.post0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (1.3.0.post0)\n",
      "Collecting trio==0.27.0 (from seleniumbase)\n",
      "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: trio-websocket==0.11.1 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (0.11.1)\n",
      "Requirement already satisfied: wsproto==1.2.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (1.2.0)\n",
      "Requirement already satisfied: websocket-client==1.8.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (1.8.0)\n",
      "Requirement already satisfied: selenium==4.27.1 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (4.27.1)\n",
      "Collecting cssselect==1.2.0 (from seleniumbase)\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: sortedcontainers==2.4.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (2.4.0)\n",
      "Collecting execnet==2.1.1 (from seleniumbase)\n",
      "  Downloading execnet-2.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting iniconfig==2.0.0 (from seleniumbase)\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pluggy==1.5.0 (from seleniumbase)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting pytest==8.3.4 (from seleniumbase)\n",
      "  Downloading pytest-8.3.4-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pytest-html==4.0.2 (from seleniumbase)\n",
      "  Downloading pytest_html-4.0.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pytest-metadata==3.1.1 (from seleniumbase)\n",
      "  Downloading pytest_metadata-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pytest-ordering==0.6 (from seleniumbase)\n",
      "  Downloading pytest_ordering-0.6-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pytest-rerunfailures==15.0 (from seleniumbase)\n",
      "  Downloading pytest_rerunfailures-15.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting pytest-xdist==3.6.1 (from seleniumbase)\n",
      "  Downloading pytest_xdist-3.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting parameterized==0.9.0 (from seleniumbase)\n",
      "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting behave==1.2.6 (from seleniumbase)\n",
      "  Downloading behave-1.2.6-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: soupsieve==2.6 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (2.6)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (4.12.3)\n",
      "Collecting pyotp==2.9.0 (from seleniumbase)\n",
      "  Downloading pyotp-2.9.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: markdown-it-py==3.0.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (3.0.0)\n",
      "Requirement already satisfied: mdurl==0.1.2 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from seleniumbase) (0.1.2)\n",
      "Collecting rich==13.9.4 (from seleniumbase)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tomli>=1 (from pytest==8.3.4->seleniumbase)\n",
      "  Downloading tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from trio==0.27.0->seleniumbase) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from cffi>=1.14->trio==0.27.0->seleniumbase) (2.22)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium==4.27.1->seleniumbase) (1.7.1)\n",
      "Downloading seleniumbase-4.33.12-py3-none-any.whl (618 kB)\n",
      "   ---------------------------------------- 0.0/618.7 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 122.9/618.7 kB 3.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 184.3/618.7 kB 2.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 245.8/618.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 307.2/618.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 368.6/618.7 kB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 430.1/618.7 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  614.4/618.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 618.7/618.7 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading behave-1.2.6-py2.py3-none-any.whl (136 kB)\n",
      "   ---------------------------------------- 0.0/136.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 136.8/136.8 kB 7.9 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-win_amd64.whl (102 kB)\n",
      "   ---------------------------------------- 0.0/102.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 102.8/102.8 kB ? eta 0:00:00\n",
      "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading execnet-2.1.1-py3-none-any.whl (40 kB)\n",
      "   ---------------------------------------- 0.0/40.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 40.6/40.6 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "   ---------------------------------------- 0.0/70.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 70.4/70.4 kB 3.8 MB/s eta 0:00:00\n",
      "Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading pyotp-2.9.0-py3-none-any.whl (13 kB)\n",
      "Downloading pytest-8.3.4-py3-none-any.whl (343 kB)\n",
      "   ---------------------------------------- 0.0/343.1 kB ? eta -:--:--\n",
      "   ------------------------------------- - 327.7/343.1 kB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 343.1/343.1 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading pytest_html-4.0.2-py3-none-any.whl (23 kB)\n",
      "Downloading pytest_metadata-3.1.1-py3-none-any.whl (11 kB)\n",
      "Downloading pytest_ordering-0.6-py3-none-any.whl (4.6 kB)\n",
      "Downloading pytest_rerunfailures-15.0-py3-none-any.whl (13 kB)\n",
      "Downloading pytest_xdist-3.6.1-py3-none-any.whl (46 kB)\n",
      "   ---------------------------------------- 0.0/46.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.1/46.1 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB ? eta 0:00:00\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "   ---------------------------------------- 0.0/242.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 242.4/242.4 kB 14.5 MB/s eta 0:00:00\n",
      "Downloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.7 kB ? eta -:--:--\n",
      "   ------------------------------------ -- 450.6/481.7 kB 13.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 481.7/481.7 kB 10.0 MB/s eta 0:00:00\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 164.9/164.9 kB ? eta 0:00:00\n",
      "Downloading exceptiongroup-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.6/134.6 kB ? eta 0:00:00\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Downloading mycdp-1.1.0-py3-none-any.whl (248 kB)\n",
      "   ---------------------------------------- 0.0/248.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 248.5/248.5 kB 15.9 MB/s eta 0:00:00\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
      "Downloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading parse_type-0.6.4-py2.py3-none-any.whl (27 kB)\n",
      "Downloading pdbp-1.6.1-py3-none-any.whl (21 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.9/1.8 MB 29.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 23.2 MB/s eta 0:00:00\n",
      "Downloading platformdirs-4.3.6-py3-none-any.whl (18 kB)\n",
      "Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 1.1/1.2 MB 22.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 25.9 MB/s eta 0:00:00\n",
      "Downloading pynose-1.5.3-py3-none-any.whl (130 kB)\n",
      "   ---------------------------------------- 0.0/130.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 130.5/130.5 kB 7.5 MB/s eta 0:00:00\n",
      "Downloading pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "   ---------------------------------------- 0.0/83.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 83.2/83.2 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "   ---------------------------------------- 0.0/161.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 161.8/161.8 kB ? eta 0:00:00\n",
      "Downloading sbvirtualdisplay-1.4.0-py3-none-any.whl (12 kB)\n",
      "Downloading setuptools-75.7.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.2/1.2 MB 39.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 25.8 MB/s eta 0:00:00\n",
      "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading tabcompleter-1.4.0-py3-none-any.whl (6.7 kB)\n",
      "Downloading websockets-14.1-cp310-cp310-win_amd64.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 163.3/163.3 kB ? eta 0:00:00\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading tomli-2.2.1-py3-none-any.whl (14 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\RoG\\anaconda3\\envs\\licenta\\python.exe -m pip install seleniumbase\n"
     ]
    }
   ],
   "source": [
    "!pip install seleniumbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sbase' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!sbase install chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.litcharts.com/lit/heart-of-darkness/part-1\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url, headers=headers)\n",
    "print(response.status_code)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "summaries = soup.find_all(\"div\", class_=\"summary-text readable highlightable-content non-paywall\")\n",
    "analysis = soup.find_all(\"div\", class_=\"analysis-text highlightable-content non-paywall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_texts = [summary.text.strip() for summary in summaries]\n",
    "analysis_texts = [analyse.text.strip() for analyse in analysis]\n",
    "HoD = pd.DataFrame({\"Summary\": summary_texts, \"Analysis\": analysis_texts})\n",
    "HoD.to_csv(\"HoD.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Summary  \\\n",
      "0  The Narrator describes the scene from the deck...   \n",
      "\n",
      "                                            Analysis  \n",
      "0  The opening establishes a dark tone. Water is ...  \n"
     ]
    }
   ],
   "source": [
    "# print first row of the dataframe\n",
    "print(HoD.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "part_index = 0\n",
    "\n",
    "while True:\n",
    "    part_index += 1\n",
    "    url = f\"https://www.litcharts.com/lit/heart-of-darkness/part-{part_index}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        break\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    summaries = soup.find_all(\"div\", class_=\"summary-text readable highlightable-content non-paywall\")\n",
    "    analysis = soup.find_all(\"div\", class_=\"analysis-text highlightable-content non-paywall\")\n",
    "    summary_texts = [summary.text.strip() for summary in summaries]\n",
    "    analysis_texts = [analyse.text.strip() for analyse in analysis]\n",
    "    HoD = pd.DataFrame({\"Summary\": summary_texts, \"Analysis\": analysis_texts})\n",
    "    HoD.to_csv(\"HoD.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Summary  \\\n",
      "0  The Narrator describes the scene from the deck...   \n",
      "\n",
      "                                            Analysis  \n",
      "0  The opening establishes a dark tone. Water is ...  \n"
     ]
    }
   ],
   "source": [
    "print(HoD.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page. Status code: 202\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page\n",
    "url = \"https://www.litcharts.com/lit/all-my-sons\"  # Replace with the actual page URL\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Send a request to the page\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    dropdown_div = soup.find('div', class_='component dropdown summary-sections')\n",
    "    #print(dropdown_div)\n",
    "\n",
    "    # Locate the dropdown menu\n",
    "    dropdown_menu = dropdown_div.find('ul', class_='dropdown-menu')\n",
    "    \n",
    "    if dropdown_menu:\n",
    "        # Extract all <a> tags inside the dropdown menu\n",
    "        dropdown_items = dropdown_menu.find_all('a')\n",
    "        \n",
    "        # Get the text from each <a> tag\n",
    "        acts = [item.get_text(strip=True) for item in dropdown_items]\n",
    "        print(acts)  # Output: ['Act 1', 'Act 2', 'Act 3']\n",
    "    else:\n",
    "        print(\"Dropdown menu not found on the page.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_summary_sections(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        dropdown_div = soup.find('div', class_='component dropdown summary-sections')\n",
    "        if dropdown_div:\n",
    "            dropdown_menu = dropdown_div.find('ul', class_='dropdown-menu')\n",
    "            if dropdown_menu:\n",
    "                dropdown_items = dropdown_menu.find_all('a')\n",
    "                sections = [\n",
    "                            re.sub(r'[^\\w\\s-]', '', item.get_text(strip=True).lower()).replace(\" \", \"-\")\n",
    "                            for item in dropdown_items\n",
    "                        ]       \n",
    "                return sections\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.litcharts.com/lit/heart-of-darkness\"\n",
    "sections = get_summary_sections(url)\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_summary_and_analysis(title, timeout=30, max_parts=0):\n",
    "    url = f\"https://www.litcharts.com/lit/{title}\"\n",
    "    sections = get_summary_sections(url)\n",
    "    time.sleep(timeout)\n",
    "    if sections:\n",
    "        index = 0\n",
    "        if max_parts == 0:\n",
    "            max_parts = len(sections)\n",
    "        for section in sections:\n",
    "            if index == max_parts:\n",
    "                break\n",
    "            index += 1\n",
    "            section_url = f\"{url}/{section}\"\n",
    "            response = requests.get(section_url, headers=headers)\n",
    "            time.sleep(timeout)\n",
    "            print(response.status_code)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                summaries = soup.find_all(\"div\", class_=\"summary-text readable highlightable-content non-paywall\")\n",
    "                analysis = soup.find_all(\"div\", class_=\"analysis-text highlightable-content non-paywall\")\n",
    "                summary_texts = [summary.text.strip() for summary in summaries]\n",
    "                analysis_texts = [analyse.text.strip() for analyse in analysis]\n",
    "                testDF = pd.DataFrame({\"Summary\": summary_texts, \"Analysis\": analysis_texts, \"Title\": title * len(summary_texts)})\n",
    "                testDF.to_csv(\"test.csv\", mode='a', header=False, index=False)\n",
    "            elif response.status_code == 202:\n",
    "                time.sleep(timeout)\n",
    "                response = requests.get(section_url, headers=headers)\n",
    "                print(response.status_code)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                    summaries = soup.find_all(\"div\", class_=\"summary-text readable highlightable-content non-paywall\")\n",
    "                    analysis = soup.find_all(\"div\", class_=\"analysis-text highlightable-content non-paywall\")\n",
    "                    summary_texts = [summary.text.strip() for summary in summaries]\n",
    "                    analysis_texts = [analyse.text.strip() for analyse in analysis]\n",
    "                    testDF = pd.DataFrame({\"Summary\": summary_texts, \"Analysis\": analysis_texts, \"Title\": title * len(summary_texts)})\n",
    "                    testDF.to_csv(\"test.csv\", mode='a', header=False, index=False)\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "202\n"
     ]
    }
   ],
   "source": [
    "title = \"the-idiot\"\n",
    "get_summary_and_analysis(title, timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-1\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-2\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-3\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-4\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-5\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-6\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-7\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-8\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-9\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-10\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-11\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-12\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-13\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-14\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-15\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-16\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-17\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-18\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-19\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-20\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-21\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-22\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-23\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-24\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-25\n",
      "Getting data for section: /lit/the-catcher-in-the-rye/chapter-26\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "# service = Service('D:\\Chromedriver\\chromedriver-win64')\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def get_summary_sections(url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    dropdown_div = soup.find('div', class_='component dropdown summary-sections')\n",
    "    if dropdown_div:\n",
    "        dropdown_menu = dropdown_div.find('ul', class_='dropdown-menu')\n",
    "        if dropdown_menu:\n",
    "            dropdown_items = dropdown_menu.find_all('a')\n",
    "            sections = [item.get('href') for item in dropdown_items if item.get('href')]    \n",
    "            return sections\n",
    "    return None\n",
    "\n",
    "def get_summary_and_analysis(title, timeout=30, max_parts=0):\n",
    "    url = f\"https://www.litcharts.com/lit/{title}\"\n",
    "    sections = get_summary_sections(url)\n",
    "    time.sleep(timeout)\n",
    "    if sections:\n",
    "        index = 0\n",
    "        if max_parts == 0:\n",
    "            max_parts = len(sections)\n",
    "        for section in sections:\n",
    "            if index == max_parts:\n",
    "                break\n",
    "            index += 1\n",
    "            print(f\"Getting data for section: {section}\")\n",
    "            section_url = f\"https://www.litcharts.com{section}\"\n",
    "            driver.get(section_url)\n",
    "            time.sleep(5) \n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            summaries = soup.find_all(\"div\", class_=\"summary-text readable highlightable-content non-paywall\")\n",
    "            analysis = soup.find_all(\"div\", class_=\"analysis-text highlightable-content non-paywall\")\n",
    "            summary_texts = [summary.text.strip() for summary in summaries]\n",
    "            analysis_texts = [analyse.text.strip() for analyse in analysis]\n",
    "            \n",
    "            if summary_texts and analysis_texts:\n",
    "                testDF = pd.DataFrame({\n",
    "                    \"Summary\": summary_texts,\n",
    "                    \"Analysis\": analysis_texts,\n",
    "                    \"Title\": [title] * len(summary_texts)\n",
    "                })\n",
    "                testDF.to_csv(\"test_selenium.csv\", mode='a', header=False, index=False)\n",
    "            else:\n",
    "                print(f\"No summaries or analysis found for section: {section}\")\n",
    "\n",
    "try:\n",
    "    get_summary_and_analysis(\"the-catcher-in-the-rye\", timeout=10, max_parts=0)\n",
    "finally:\n",
    "    driver.quit()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No chapter scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for section: the-killers\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "# service = Service('D:\\Chromedriver\\chromedriver-win64')\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def get_summary_and_analysis_no_chapters(title, timeout=30):\n",
    "    print(f\"Getting data for section: {title}\")\n",
    "    section_url = f\"https://www.litcharts.com/lit/{title}/summary-and-analysis\"\n",
    "    driver.get(section_url)\n",
    "    time.sleep(5) \n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    summaries = soup.find_all(\"div\", class_=\"summary-text readable highlightable-content non-paywall\")\n",
    "    analysis = soup.find_all(\"div\", class_=\"analysis-text highlightable-content non-paywall\")\n",
    "    summary_texts = [summary.text.strip() for summary in summaries]\n",
    "    analysis_texts = [analyse.text.strip() for analyse in analysis]\n",
    "    \n",
    "    if summary_texts and analysis_texts:\n",
    "        testDF = pd.DataFrame({\n",
    "            \"Summary\": summary_texts,\n",
    "            \"Analysis\": analysis_texts,\n",
    "            \"Title\": [title] * len(summary_texts)\n",
    "        })\n",
    "        testDF.to_csv(\"test_selenium.csv\", mode='a', header=False, index=False)\n",
    "    else:\n",
    "        print(f\"No summaries or analysis found for section: {title}\")\n",
    "\n",
    "try:\n",
    "    get_summary_and_analysis_no_chapters(\"the-killers\", timeout=10)\n",
    "finally:\n",
    "    driver.quit()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file without headers\n",
    "df = pd.read_csv(\"test_selenium.csv\", header=None)\n",
    "\n",
    "# Assign column names (replace with your actual column names)\n",
    "df.columns = [\"Summary\", \"Analysis\", \"Title\"]\n",
    "\n",
    "# Save the updated CSV with headers\n",
    "df.to_csv(\"test_selenium.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
