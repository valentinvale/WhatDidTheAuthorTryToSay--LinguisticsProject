{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rog\\appdata\\roaming\\python\\python310\\site-packages (from selenium) (2023.11.17)\n",
      "Collecting typing_extensions~=4.9 (from selenium)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\rog\\appdata\\roaming\\python\\python310\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/9.7 MB 1.7 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/9.7 MB 2.8 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.7/9.7 MB 3.8 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/9.7 MB 4.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/9.7 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.1/9.7 MB 10.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.6/9.7 MB 16.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.3/9.7 MB 17.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.2/9.7 MB 18.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.6/9.7 MB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 18.3 MB/s eta 0:00:00\n",
      "Downloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
      "   ---------------------------------------- 0.0/486.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 486.3/486.3 kB 31.7 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 58.8/58.8 kB ? eta 0:00:00\n",
      "Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.4/63.4 kB ? eta 0:00:00\n",
      "Downloading cffi-1.17.1-cp310-cp310-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.3/181.3 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, websocket-client, typing_extensions, sniffio, pysocks, pycparser, h11, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 1.7.0\n",
      "    Uninstalling websocket-client-1.7.0:\n",
      "      Successfully uninstalled websocket-client-1.7.0\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-24.3.0 cffi-1.17.1 h11-0.14.0 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 selenium-4.27.1 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 typing_extensions-4.12.2 websocket-client-1.8.0 wsproto-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.2 requires torch==2.2.2, but you have torch 2.0.1 which is incompatible.\n",
      "torchvision 0.17.2 requires torch==2.2.2, but you have torch 2.0.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.litcharts.com/lit/heart-of-darkness/part-1\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url, headers=headers)\n",
    "print(response.status_code)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "summaries = soup.find_all(\"div\", class_=\"summary-text readable highlightable-content non-paywall\")\n",
    "analysis = soup.find_all(\"div\", class_=\"analysis-text highlightable-content non-paywall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_texts = [summary.text.strip() for summary in summaries]\n",
    "analysis_texts = [analyse.text.strip() for analyse in analysis]\n",
    "HoD = pd.DataFrame({\"Summary\": summary_texts, \"Analysis\": analysis_texts})\n",
    "HoD.to_csv(\"HoD.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Summary  \\\n",
      "0  The Narrator describes the scene from the deck...   \n",
      "\n",
      "                                            Analysis  \n",
      "0  The opening establishes a dark tone. Water is ...  \n"
     ]
    }
   ],
   "source": [
    "# print first row of the dataframe\n",
    "print(HoD.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    }
   ],
   "source": [
    "part_index = 0\n",
    "\n",
    "while True:\n",
    "    part_index += 1\n",
    "    url = f\"https://www.litcharts.com/lit/heart-of-darkness/part-{part_index}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        break\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    summaries = soup.find_all(\"div\", class_=\"summary-text readable highlightable-content non-paywall\")\n",
    "    analysis = soup.find_all(\"div\", class_=\"analysis-text highlightable-content non-paywall\")\n",
    "    summary_texts = [summary.text.strip() for summary in summaries]\n",
    "    analysis_texts = [analyse.text.strip() for analyse in analysis]\n",
    "    HoD = pd.DataFrame({\"Summary\": summary_texts, \"Analysis\": analysis_texts})\n",
    "    HoD.to_csv(\"HoD.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Summary  \\\n",
      "0  The Narrator describes the scene from the deck...   \n",
      "\n",
      "                                            Analysis  \n",
      "0  The opening establishes a dark tone. Water is ...  \n"
     ]
    }
   ],
   "source": [
    "print(HoD.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page. Status code: 202\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page\n",
    "url = \"https://www.litcharts.com/lit/all-my-sons\"  # Replace with the actual page URL\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Send a request to the page\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    dropdown_div = soup.find('div', class_='component dropdown summary-sections')\n",
    "    #print(dropdown_div)\n",
    "\n",
    "    # Locate the dropdown menu\n",
    "    dropdown_menu = dropdown_div.find('ul', class_='dropdown-menu')\n",
    "    \n",
    "    if dropdown_menu:\n",
    "        # Extract all <a> tags inside the dropdown menu\n",
    "        dropdown_items = dropdown_menu.find_all('a')\n",
    "        \n",
    "        # Get the text from each <a> tag\n",
    "        acts = [item.get_text(strip=True) for item in dropdown_items]\n",
    "        print(acts)  # Output: ['Act 1', 'Act 2', 'Act 3']\n",
    "    else:\n",
    "        print(\"Dropdown menu not found on the page.\")\n",
    "else:\n",
    "    print(f\"Failed to fetch page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_sections(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(response.status_code)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        dropdown_div = soup.find('div', class_='component dropdown summary-sections')\n",
    "        if dropdown_div:\n",
    "            dropdown_menu = dropdown_div.find('ul', class_='dropdown-menu')\n",
    "            if dropdown_menu:\n",
    "                dropdown_items = dropdown_menu.find_all('a')\n",
    "                sections = [item.get_text(strip=True).lower().replace(\" \", \"-\") for item in dropdown_items]\n",
    "                return sections\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.litcharts.com/lit/heart-of-darkness\"\n",
    "sections = get_summary_sections(url)\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_summary_and_analysis(title, timeout=30):\n",
    "    url = f\"https://www.litcharts.com/lit/{title}\"\n",
    "    sections = get_summary_sections(url)\n",
    "    time.sleep(timeout)\n",
    "    if sections:\n",
    "        for section in sections:\n",
    "            section_url = f\"{url}/{section}\"\n",
    "            response = requests.get(section_url, headers=headers)\n",
    "            time.sleep(timeout)\n",
    "            print(response.status_code)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                summaries = soup.find_all(\"div\", class_=\"summary-text readable highlightable-content non-paywall\")\n",
    "                analysis = soup.find_all(\"div\", class_=\"analysis-text highlightable-content non-paywall\")\n",
    "                summary_texts = [summary.text.strip() for summary in summaries]\n",
    "                analysis_texts = [analyse.text.strip() for analyse in analysis]\n",
    "                testDF = pd.DataFrame({\"Summary\": summary_texts, \"Analysis\": analysis_texts})\n",
    "                testDF.to_csv(\"test.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "title = \"heart-of-darkness\"\n",
    "get_summary_and_analysis(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
