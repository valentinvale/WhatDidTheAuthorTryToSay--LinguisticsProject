{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('test_selenium.csv')\n",
    "summaries = data['Summary']\n",
    "analyses = data['Analysis']\n",
    "titles = data['Title']\n",
    "\n",
    "corpus = [f\"{title} {summary} {analysis}\" for title, summary, analysis in zip(titles, summaries, analyses)]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "preprocessed_corpus = [preprocess_text(text) for text in corpus]\n",
    "\n",
    "tokenized_corpus = [text.split() for text in preprocessed_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word Embedding Model To Obtain Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "Word2Vec_model_sg = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "Word2Vec_model_sg.save('Word2Vec_model_sg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.18757609 -0.02523065  0.10822345  0.25279427  0.09404436 -0.14556998\n",
      "  0.31167802  0.56639236 -0.07748945 -0.11939991 -0.2481058  -0.37848467\n",
      " -0.0188977  -0.1670922  -0.10497189 -0.08356968 -0.0615345   0.10890697\n",
      " -0.10079912 -0.40617284 -0.00819069 -0.1524098   0.16265742 -0.03898332\n",
      " -0.06940196  0.04204768 -0.03904338 -0.06583715 -0.15557028  0.20045884\n",
      "  0.3325572   0.00232724  0.14604664 -0.25277588 -0.20083642  0.21060328\n",
      "  0.01625244 -0.34341818 -0.21040498 -0.30221328  0.05520817 -0.08441182\n",
      "  0.0313545   0.01122725  0.27458957 -0.2581353  -0.3079668  -0.02439034\n",
      " -0.22528283  0.05035021 -0.07226291 -0.00409833 -0.03894206  0.06516527\n",
      "  0.09041492 -0.08996686  0.08819439 -0.06942315  0.07562473  0.10474674\n",
      "  0.00300599 -0.16347326  0.36412808  0.02851762 -0.01989603  0.17973474\n",
      "  0.06861334  0.25913286 -0.15470749  0.06786061 -0.00504779  0.20889491\n",
      "  0.2030788  -0.25446317  0.15321966  0.13806863  0.31823     0.15149638\n",
      " -0.43128183 -0.07699223 -0.12870276 -0.00841347  0.10805228  0.12347965\n",
      "  0.12361367  0.10686838  0.18896607  0.06795093  0.2968159   0.09123122\n",
      "  0.21289487  0.4078225   0.00973898  0.17752181  0.34776276 -0.07055833\n",
      " -0.01305874 -0.35923305  0.16940497  0.01615659]\n"
     ]
    }
   ],
   "source": [
    "test_vector = Word2Vec_model_sg.wv['darkness']\n",
    "print(test_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('brother', 0.8718932867050171), ('wife', 0.8443353176116943), ('servant', 0.8382858633995056), ('sister', 0.8214103579521179), ('aunt', 0.8185446858406067), ('girlfriend', 0.8092584609985352), ('acquaintance', 0.802879810333252), ('grandfather', 0.8003736138343811), ('roommate', 0.7951571345329285), ('companion', 0.7915104627609253)]\n"
     ]
    }
   ],
   "source": [
    "similar_words = Word2Vec_model_sg.wv.most_similar('friend')\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar entry: the-killers George brings Max and Al their meals, but they can‚Äôt remember who ordered what. As they eat, they catch George looking at them. Al suggests that ‚Äúmaybe the boy meant it for a joke,‚Äù and George laughs. Max tells him not to laugh and George says alright. To Al, Max says ‚Äúhe thinks it‚Äôs all right,‚Äù and Al replies, ‚ÄúOh, he‚Äôs a thinker.‚Äù Max and Al want to seem like they are in control of the situation and know what they are doing, but their confusion over who ordered what reveals how easy it is to fluster them (and shows them to be either a little stupid or unobservant‚Äîan inauspicious beginning for criminals). Frustrated by this confusion, they again emasculate George to put him in his place as their inferior.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def sentence_vector(sentence, model):\n",
    "    words = sentence.split()\n",
    "    valid_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(valid_vectors, axis=0) if valid_vectors else np.zeros(model.vector_size)\n",
    "\n",
    "user_input = \"I am feeling very sad and lonely. I don't have any friends to talk to.\"\n",
    "preprocessed_input = preprocess_text(user_input)\n",
    "input_vector = sentence_vector(preprocessed_input, Word2Vec_model_sg)\n",
    "\n",
    "corpus_vectors = [sentence_vector(entry, Word2Vec_model_sg) for entry in preprocessed_corpus]\n",
    "similarities = cosine_similarity([input_vector], corpus_vectors)\n",
    "\n",
    "most_similar_index = np.argmax(similarities)\n",
    "print(f\"Most similar entry: {corpus[most_similar_index]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('test_selenium.csv')\n",
    "data['Input'] = data['Summary']\n",
    "data['Output'] = data['Analysis']\n",
    "\n",
    "data[['Input', 'Output']].to_csv('fine_tuning_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 5095 examples [00:00, 51968.66 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4126\n",
      "Validation size: 459\n",
      "Test size: 510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files={\"train\": 'fine_tuning_data.csv'})\n",
    "\n",
    "split = dataset['train'].train_test_split(test_size=0.1)\n",
    "train_dataset = split['train']\n",
    "test_dataset = split['test']\n",
    "\n",
    "split = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split['train']\n",
    "val_dataset = split['test']\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4126/4126 [00:03<00:00, 1352.93 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 459/459 [00:00<00:00, 1203.57 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 510/510 [00:00<00:00, 1285.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Check for empty inputs or outputs\n",
    "    if \"Input\" not in examples or \"Output\" not in examples:\n",
    "        raise ValueError(\"Input or output column missing in dataset.\")\n",
    "    \n",
    "    # Ensure input and output are non-empty strings\n",
    "    inputs = examples[\"Input\"]  # Replace with your dataset's input column\n",
    "    targets = examples[\"Output\"]  # Replace with your dataset's output column\n",
    "    \n",
    "    if not inputs or not targets:\n",
    "        raise ValueError(\"Empty input or output found in the dataset.\")\n",
    "\n",
    "    # Tokenize inputs and outputs\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Reapply preprocessing\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "from torch import cuda\n",
    "print(cuda.is_available())  # Should return True if GPU is available\n",
    "\n",
    "cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_23708\\1805935233.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")  # Replace with the appropriate metric for your task\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"rouge\")  # Replace with the appropriate metric for your task\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects a list of strings\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {key: value.mid.fmeasure for key, value in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1031/3093 [09:19<18:38,  1.84it/s]\n",
      "  6%|‚ñã         | 200/3093 [00:46<10:56,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2385, 'grad_norm': 0.6222633123397827, 'learning_rate': 4.6766892984157775e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 400/3093 [01:33<10:42,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2009, 'grad_norm': 0.702217161655426, 'learning_rate': 4.3533785968315555e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñâ        | 600/3093 [02:20<09:48,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1696, 'grad_norm': 0.6178997755050659, 'learning_rate': 4.030067895247333e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñå       | 800/3093 [03:07<09:05,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1847, 'grad_norm': 0.7727011442184448, 'learning_rate': 3.706757193663111e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 1000/3093 [03:55<08:40,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2205, 'grad_norm': 0.6378457546234131, 'learning_rate': 3.383446492078888e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1031/3093 [04:03<08:21,  4.11it/s]\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1031/3093 [06:04<08:21,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1342835426330566, 'eval_rouge1': 0.14481260142274638, 'eval_rouge2': 0.027660204847849115, 'eval_rougeL': 0.1178565143934396, 'eval_rougeLsum': 0.11788527549730675, 'eval_runtime': 120.8576, 'eval_samples_per_second': 3.798, 'eval_steps_per_second': 3.798, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|‚ñà‚ñà‚ñà‚ñâ      | 1200/3093 [06:45<07:36,  4.15it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2068, 'grad_norm': 0.7466623187065125, 'learning_rate': 3.0601357904946654e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1400/3093 [07:33<06:53,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1732, 'grad_norm': 1.4678646326065063, 'learning_rate': 2.736825088910443e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1600/3093 [08:22<05:50,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1821, 'grad_norm': 0.46581369638442993, 'learning_rate': 2.4135143873262206e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1800/3093 [09:10<05:02,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1682, 'grad_norm': 0.605596125125885, 'learning_rate': 2.0902036857419983e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 2000/3093 [09:57<04:19,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1023, 'grad_norm': 0.568564772605896, 'learning_rate': 1.766892984157776e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2063/3093 [10:12<04:08,  4.14it/s]c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2063/3093 [12:01<04:08,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1275582313537598, 'eval_rouge1': 0.15079385505966109, 'eval_rouge2': 0.029024563117201262, 'eval_rougeL': 0.12142024200429173, 'eval_rougeLsum': 0.12138241741218958, 'eval_runtime': 109.2471, 'eval_samples_per_second': 4.201, 'eval_steps_per_second': 4.201, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 2200/3093 [12:35<03:42,  4.01it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1564, 'grad_norm': 0.4658587574958801, 'learning_rate': 1.4435822825735532e-05, 'epoch': 2.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 2400/3093 [13:22<02:48,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1353, 'grad_norm': 0.6698911786079407, 'learning_rate': 1.1202715809893308e-05, 'epoch': 2.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2600/3093 [14:09<01:58,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.139, 'grad_norm': 0.6918927431106567, 'learning_rate': 7.969608794051083e-06, 'epoch': 2.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2800/3093 [14:57<01:09,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1572, 'grad_norm': 0.891018807888031, 'learning_rate': 4.736501778208859e-06, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 3000/3093 [15:44<00:21,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1589, 'grad_norm': 0.5955147743225098, 'learning_rate': 1.5033947623666343e-06, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3093/3093 [16:07<00:00,  4.13it/s]c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3093/3093 [17:59<00:00,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1250534057617188, 'eval_rouge1': 0.15096450782852516, 'eval_rouge2': 0.028817421582187856, 'eval_rougeL': 0.12104128063610468, 'eval_rougeLsum': 0.12105277787846327, 'eval_runtime': 112.6125, 'eval_samples_per_second': 4.076, 'eval_steps_per_second': 4.076, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3093/3093 [18:01<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1081.2941, 'train_samples_per_second': 11.447, 'train_steps_per_second': 2.86, 'train_loss': 1.2379047900698466, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_t5_tokenizer\\\\tokenizer_config.json',\n",
       " 'fine_tuned_t5_tokenizer\\\\special_tokens_map.json',\n",
       " 'fine_tuned_t5_tokenizer\\\\spiece.model',\n",
       " 'fine_tuned_t5_tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Clear GPU cache and set memory configuration\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./fine_tuned_t5\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",        # Save at the end of each epoch\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=1,  # Reduce batch size to fit in memory\n",
    "    per_device_eval_batch_size=1,  # Match eval batch size with training batch size\n",
    "    gradient_accumulation_steps=4,  # Simulate larger batch size (effective batch size = 1 * 4 = 4)\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,             # Log less frequently to reduce console clutter\n",
    "    fp16=False,                    # Set to True if your GPU supports mixed precision\n",
    "    predict_with_generate=True,    # Enable text generation for evaluation\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "# Use Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics  # Custom evaluation metrics\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('fine_tuned_t5')\n",
    "tokenizer.save_pretrained('fine_tuned_t5_tokenizer')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
